{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cf5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\chhaya jangid\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.14.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in c:\\users\\chhaya jangid\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\chhaya jangid\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Chhaya Jangid\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Scraping data for new-delhi\n",
      "==================================================\n",
      "Scraping new-delhi - Page 1\n",
      "Scraping new-delhi - Page 2\n",
      "Scraping new-delhi - Page 3\n",
      "Scraping new-delhi - Page 4\n",
      "Scraping new-delhi - Page 5\n",
      "Scraping new-delhi - Page 6\n",
      "Scraping new-delhi - Page 7\n",
      "Scraping new-delhi - Page 8\n",
      "Scraping new-delhi - Page 9\n",
      "Scraping new-delhi - Page 10\n",
      "Scraping new-delhi - Page 11\n",
      "Saved raw data for new-delhi with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for mumbai\n",
      "==================================================\n",
      "Scraping mumbai - Page 1\n",
      "Scraping mumbai - Page 2\n",
      "Scraping mumbai - Page 3\n",
      "Scraping mumbai - Page 4\n",
      "Scraping mumbai - Page 5\n",
      "Scraping mumbai - Page 6\n",
      "Scraping mumbai - Page 7\n",
      "Scraping mumbai - Page 8\n",
      "Scraping mumbai - Page 9\n",
      "Scraping mumbai - Page 10\n",
      "Scraping mumbai - Page 11\n",
      "Saved raw data for mumbai with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for pune\n",
      "==================================================\n",
      "Scraping pune - Page 1\n",
      "Scraping pune - Page 2\n",
      "Scraping pune - Page 3\n",
      "Scraping pune - Page 4\n",
      "Scraping pune - Page 5\n",
      "Scraping pune - Page 6\n",
      "Scraping pune - Page 7\n",
      "Scraping pune - Page 8\n",
      "Scraping pune - Page 9\n",
      "Scraping pune - Page 10\n",
      "Scraping pune - Page 11\n",
      "Saved raw data for pune with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for bangalore\n",
      "==================================================\n",
      "Scraping bangalore - Page 1\n",
      "Scraping bangalore - Page 2\n",
      "Scraping bangalore - Page 3\n",
      "Scraping bangalore - Page 4\n",
      "Scraping bangalore - Page 5\n",
      "Scraping bangalore - Page 6\n",
      "Scraping bangalore - Page 7\n",
      "Scraping bangalore - Page 8\n",
      "Scraping bangalore - Page 9\n",
      "Scraping bangalore - Page 10\n",
      "Scraping bangalore - Page 11\n",
      "Saved raw data for bangalore with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for chennai\n",
      "==================================================\n",
      "Scraping chennai - Page 1\n",
      "Scraping chennai - Page 2\n",
      "Scraping chennai - Page 3\n",
      "Scraping chennai - Page 4\n",
      "Scraping chennai - Page 5\n",
      "Scraping chennai - Page 6\n",
      "Scraping chennai - Page 7\n",
      "Scraping chennai - Page 8\n",
      "Scraping chennai - Page 9\n",
      "Scraping chennai - Page 10\n",
      "Scraping chennai - Page 11\n",
      "Saved raw data for chennai with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for noida\n",
      "==================================================\n",
      "Scraping noida - Page 1\n",
      "Scraping noida - Page 2\n",
      "Scraping noida - Page 3\n",
      "Scraping noida - Page 4\n",
      "Scraping noida - Page 5\n",
      "Scraping noida - Page 6\n",
      "Scraping noida - Page 7\n",
      "Scraping noida - Page 8\n",
      "Scraping noida - Page 9\n",
      "Scraping noida - Page 10\n",
      "Scraping noida - Page 11\n",
      "Saved raw data for noida with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for jaipur\n",
      "==================================================\n",
      "Scraping jaipur - Page 1\n",
      "Scraping jaipur - Page 2\n",
      "Scraping jaipur - Page 3\n",
      "Scraping jaipur - Page 4\n",
      "Scraping jaipur - Page 5\n",
      "Scraping jaipur - Page 6\n",
      "Scraping jaipur - Page 7\n",
      "Scraping jaipur - Page 8\n",
      "Scraping jaipur - Page 9\n",
      "Scraping jaipur - Page 10\n",
      "Scraping jaipur - Page 11\n",
      "Saved raw data for jaipur with 220 records\n",
      "\n",
      "==================================================\n",
      "Scraping data for surat\n",
      "==================================================\n",
      "Scraping surat - Page 1\n",
      "Scraping surat - Page 2\n",
      "Scraping surat - Page 3\n",
      "Scraping surat - Page 4\n",
      "Scraping surat - Page 5\n",
      "Scraping surat - Page 6\n",
      "Scraping surat - Page 7\n",
      "Scraping surat - Page 8\n",
      "Scraping surat - Page 9\n",
      "Scraping surat - Page 10\n",
      "Scraping surat - Page 11\n",
      "Saved raw data for surat with 220 records\n",
      "\n",
      "==================================================\n",
      "Processing scraped data\n",
      "==================================================\n",
      "\n",
      "new-delhi -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 63\n",
      "\n",
      "mumbai -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 63\n",
      "\n",
      "pune -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 63\n",
      "\n",
      "bangalore -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 63\n",
      "\n",
      "chennai -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 63\n",
      "\n",
      "noida -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 62\n",
      "\n",
      "jaipur -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 61\n",
      "\n",
      "surat -------------->\n",
      "Total records: 220\n",
      "Missing in job column: 0\n",
      "Missing in salary column: 0\n",
      "Missing in founded_in column: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:200: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"job\"].fillna(median_job, inplace=True)\n",
      "C:\\Users\\Chhaya Jangid\\AppData\\Local\\Temp\\ipykernel_20384\\3157148008.py:204: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"salary\"].fillna(median_salary, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "%pip install beautifulsoup4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_salary_job(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    \n",
    "    val = str(val).strip()\n",
    "    \n",
    "    if val.isnumeric():\n",
    "        return int(val)\n",
    "    \n",
    "    if val.endswith(\"L\"):\n",
    "        val = val.replace(\"L\", \"\")\n",
    "        if val.replace('.', '', 1).isdigit():\n",
    "            return float(val) * 100000\n",
    "    \n",
    "    if val.endswith(\"k\"):\n",
    "        val = val.replace(\"k\", \"\")\n",
    "        if val.replace('.', '', 1).isdigit():\n",
    "            return float(val) * 1000\n",
    "    \n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def scrape_city_data(city, headers, max_pages=11):\n",
    "    \"\"\"Scrape company data for a specific city\"\"\"\n",
    "    url_for_city = f\"https://www.ambitionbox.com/companies-in-{city}\"\n",
    "    dictionary = {\n",
    "        \"company_name\": [],\n",
    "        \"rating\": [],\n",
    "        \"bio\": [],\n",
    "        \"salary\": [],\n",
    "        \"job\": [],\n",
    "        \"founded_in\": []\n",
    "    }\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping {city} - Page {page}\")\n",
    "        \n",
    "        try:\n",
    "            pages_url = f\"{url_for_city}?page={page}\"\n",
    "            resp = requests.get(pages_url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()  \n",
    "            \n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            company_card_wrappers = soup.find_all(\"div\", class_=\"companyCardWrapper\")\n",
    "            \n",
    "            if not company_card_wrappers:\n",
    "                print(f\"No company cards found on page {page} for {city}\")\n",
    "                break\n",
    "            \n",
    "            for data in company_card_wrappers:\n",
    "                company_name_elem = data.find(\"h2\")\n",
    "                company_name = company_name_elem.text.strip() if company_name_elem else np.nan\n",
    "                dictionary[\"company_name\"].append(company_name)\n",
    "                \n",
    "               \n",
    "                rating_elem = data.find(\"div\", class_=\"rating_text\")\n",
    "                rating = rating_elem.text.strip() if rating_elem else np.nan\n",
    "                dictionary[\"rating\"].append(rating)\n",
    "                \n",
    "                \n",
    "                bio_elem = data.find(\"span\", class_=\"companyCardWrapper__interLinking\")\n",
    "                bio = bio_elem.text.strip() if bio_elem else np.nan\n",
    "                dictionary[\"bio\"].append(bio)\n",
    "                \n",
    "            \n",
    "                if company_name and company_name != np.nan:\n",
    "                    company_slug = company_name.lower().replace(' ', '-').replace('.', '').replace(',', '')\n",
    "                    overview_url = f\"https://www.ambitionbox.com/overview/{company_slug}-overview\"\n",
    "                    \n",
    "                    try:\n",
    "                        time.sleep(1)  \n",
    "                        resp1 = requests.get(overview_url, headers=headers, timeout=10)\n",
    "                        \n",
    "                        if resp1.status_code == 200:\n",
    "                            new_soup = BeautifulSoup(resp1.content, 'html.parser')\n",
    "                            \n",
    "                            salary = np.nan\n",
    "                            a_tag_salary = new_soup.find(\"a\", title=lambda x: x and \"Salaries\" in x)\n",
    "                            if a_tag_salary:\n",
    "                                salary_elem = a_tag_salary.find(\"div\", class_=\"text-primary-text font-pn-600 text-xs\")\n",
    "                                if salary_elem:\n",
    "                                    salary = salary_elem.text.strip()\n",
    "                            dictionary[\"salary\"].append(salary)\n",
    "                            \n",
    "                        \n",
    "                            job = np.nan\n",
    "                            a_tag_job = new_soup.find(\"a\", title=lambda x: x and \"Jobs\" in x)\n",
    "                            if a_tag_job:\n",
    "                                job_elem = a_tag_job.find(\"div\", class_=\"text-primary-text font-pn-600 text-xs\")\n",
    "                                if job_elem:\n",
    "                                    job = job_elem.text.strip()\n",
    "                            dictionary[\"job\"].append(job)\n",
    "                            \n",
    "                        \n",
    "                            founded_year = np.nan\n",
    "                            year_elements = new_soup.find_all(\"div\", \n",
    "                                class_=\"inline whitespace-pre-wrap break-words text-primary-text text-sm font-pn-600 flex-[6] md:flex-[auto]\")\n",
    "                            if year_elements and len(year_elements) > 0:\n",
    "                                founded_year = year_elements[0].text.strip()\n",
    "                            dictionary[\"founded_in\"].append(founded_year)\n",
    "                            \n",
    "                        else:\n",
    "                           \n",
    "                            dictionary[\"salary\"].append(np.nan)\n",
    "                            dictionary[\"job\"].append(np.nan)\n",
    "                            dictionary[\"founded_in\"].append(np.nan)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error scraping {company_name}: {e}\")\n",
    "                        dictionary[\"salary\"].append(np.nan)\n",
    "                        dictionary[\"job\"].append(np.nan)\n",
    "                        dictionary[\"founded_in\"].append(np.nan)\n",
    "                else:\n",
    "                    dictionary[\"salary\"].append(np.nan)\n",
    "                    dictionary[\"job\"].append(np.nan)\n",
    "                    dictionary[\"founded_in\"].append(np.nan)\n",
    "            \n",
    "            time.sleep(2)  \n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error accessing page {page} for {city}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(dictionary)\n",
    "\n",
    "\n",
    "def main():\n",
    "    cities = [\"new-delhi\", \"mumbai\", \"pune\", \"bangalore\", \"chennai\", \"noida\", \"jaipur\", \"surat\"]\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "    \n",
    "   \n",
    "    output_dir = \"scraped_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    for city in cities:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Scraping data for {city}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            df = scrape_city_data(city, headers, max_pages=11)\n",
    "            \n",
    "            if not df.empty:\n",
    "              \n",
    "                raw_filename = os.path.join(output_dir, f\"{city}_raw.csv\")\n",
    "                df.to_csv(raw_filename, index=False)\n",
    "                print(f\"Saved raw data for {city} with {len(df)} records\")\n",
    "            else:\n",
    "                print(f\"No data scraped for {city}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {city}: {e}\")\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Processing scraped data\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for city in cities:\n",
    "        filepath = os.path.join(output_dir, f\"{city}_raw.csv\")\n",
    "        \n",
    "        if os.path.exists(filepath):\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                \n",
    "                if not df.empty:\n",
    "                    df[\"job\"] = df[\"job\"].apply(clean_salary_job)\n",
    "                    df[\"salary\"] = df[\"salary\"].apply(clean_salary_job)\n",
    "                    \n",
    "                   \n",
    "                    if df[\"job\"].notna().any():\n",
    "                        median_job = df[\"job\"].median()\n",
    "                        df[\"job\"].fillna(median_job, inplace=True)\n",
    "                    \n",
    "                    if df[\"salary\"].notna().any():\n",
    "                        median_salary = df[\"salary\"].median()\n",
    "                        df[\"salary\"].fillna(median_salary, inplace=True)\n",
    "                    \n",
    "                   \n",
    "                    df[[\"field\", \"other\"]] = df[\"bio\"].str.split(\"|\", n=1, expand=True)\n",
    "                \n",
    "                   \n",
    "                    df[\"field\"] = df[\"field\"].str.strip() if df[\"field\"].notna().any() else df[\"field\"]\n",
    "                    df[\"other\"] = df[\"other\"].str.strip() if df[\"other\"].notna().any() else df[\"other\"]\n",
    "                    \n",
    "                    processed_filename = os.path.join(output_dir, f\"{city}_processed.csv\")\n",
    "                    df.to_csv(processed_filename, index=False)\n",
    "                    \n",
    "                    print(f\"\\n{city} -------------->\")\n",
    "                    print(f\"Total records: {len(df)}\")\n",
    "                    print(f\"Missing in job column: {df['job'].isnull().sum()}\")\n",
    "                    print(f\"Missing in salary column: {df['salary'].isnull().sum()}\")\n",
    "                    print(f\"Missing in founded_in column: {df['founded_in'].isnull().sum()}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {city}: {e}\")\n",
    "        else:\n",
    "            print(f\"No data file found for {city}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6664c4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstyles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Font, Alignment, PatternFill, Border, Side\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_column_letter\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, Alignment, PatternFill, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def clean_salary_job(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    \n",
    "    val = str(val).strip()\n",
    "    \n",
    "    if val.isnumeric():\n",
    "        return int(val)\n",
    "    \n",
    "    if val.endswith(\"L\"):\n",
    "        val = val.replace(\"L\", \"\")\n",
    "        if val.replace('.', '', 1).isdigit():\n",
    "            return float(val) * 100000\n",
    "    \n",
    "    if val.endswith(\"k\"):\n",
    "        val = val.replace(\"k\", \"\")\n",
    "        if val.replace('.', '', 1).isdigit():\n",
    "            return float(val) * 1000\n",
    "    \n",
    "    try:\n",
    "        return float(val)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def format_excel_file(filepath, sheet_name='Sheet1'):\n",
    "    \"\"\"Apply formatting to Excel file\"\"\"\n",
    "    from openpyxl import load_workbook\n",
    "    \n",
    "    wb = load_workbook(filepath)\n",
    "    ws = wb[sheet_name]\n",
    "    \n",
    "    # Define styles\n",
    "    header_fill = PatternFill(start_color=\"366092\", end_color=\"366092\", fill_type=\"solid\")\n",
    "    header_font = Font(color=\"FFFFFF\", bold=True, size=12)\n",
    "    border = Border(\n",
    "        left=Side(style='thin'),\n",
    "        right=Side(style='thin'),\n",
    "        top=Side(style='thin'),\n",
    "        bottom=Side(style='thin')\n",
    "    )\n",
    "    \n",
    "    # Format header row\n",
    "    for cell in ws[1]:\n",
    "        cell.fill = header_fill\n",
    "        cell.font = header_font\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "        cell.border = border\n",
    "    \n",
    "    # Format data rows\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):\n",
    "        for cell in row:\n",
    "            cell.border = border\n",
    "            cell.alignment = Alignment(horizontal=\"left\", vertical=\"center\")\n",
    "    \n",
    "    # Auto-adjust column widths\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        column_letter = get_column_letter(column[0].column)\n",
    "        \n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        adjusted_width = min(max_length + 4, 50)\n",
    "        ws.column_dimensions[column_letter].width = adjusted_width\n",
    "    \n",
    "    # Format currency columns\n",
    "    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):\n",
    "        # Assuming salary is in column D (4th column)\n",
    "        salary_cell = row[3] if len(row) > 3 else None\n",
    "        if salary_cell and isinstance(salary_cell.value, (int, float)):\n",
    "            salary_cell.number_format = '\"‚Çπ\"#,##0'\n",
    "    \n",
    "    wb.save(filepath)\n",
    "\n",
    "\n",
    "def scrape_city_data(city, headers, max_pages=11):\n",
    "    \"\"\"Scrape company data for a specific city\"\"\"\n",
    "    url_for_city = f\"https://www.ambitionbox.com/companies-in-{city}\"\n",
    "    dictionary = {\n",
    "        \"company_name\": [],\n",
    "        \"rating\": [],\n",
    "        \"bio\": [],\n",
    "        \"salary\": [],\n",
    "        \"job\": [],\n",
    "        \"founded_in\": []\n",
    "    }\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        print(f\"Scraping {city} - Page {page}\")\n",
    "        \n",
    "        try:\n",
    "            pages_url = f\"{url_for_city}?page={page}\"\n",
    "            resp = requests.get(pages_url, headers=headers, timeout=10)\n",
    "            resp.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            company_card_wrappers = soup.find_all(\"div\", class_=\"companyCardWrapper\")\n",
    "            \n",
    "            if not company_card_wrappers:\n",
    "                print(f\"No company cards found on page {page} for {city}\")\n",
    "                break\n",
    "            \n",
    "            for data in company_card_wrappers:\n",
    "                # Extract company name\n",
    "                company_name_elem = data.find(\"h2\")\n",
    "                company_name = company_name_elem.text.strip() if company_name_elem else np.nan\n",
    "                dictionary[\"company_name\"].append(company_name)\n",
    "                \n",
    "                # Extract rating\n",
    "                rating_elem = data.find(\"div\", class_=\"rating_text\")\n",
    "                rating = rating_elem.text.strip() if rating_elem else np.nan\n",
    "                dictionary[\"rating\"].append(rating)\n",
    "                \n",
    "                # Extract bio\n",
    "                bio_elem = data.find(\"span\", class_=\"companyCardWrapper__interLinking\")\n",
    "                bio = bio_elem.text.strip() if bio_elem else np.nan\n",
    "                dictionary[\"bio\"].append(bio)\n",
    "                \n",
    "                # Scrape additional details\n",
    "                if company_name and company_name != np.nan:\n",
    "                    company_slug = company_name.lower().replace(' ', '-').replace('.', '').replace(',', '')\n",
    "                    overview_url = f\"https://www.ambitionbox.com/overview/{company_slug}-overview\"\n",
    "                    \n",
    "                    try:\n",
    "                        time.sleep(1)\n",
    "                        resp1 = requests.get(overview_url, headers=headers, timeout=10)\n",
    "                        \n",
    "                        if resp1.status_code == 200:\n",
    "                            new_soup = BeautifulSoup(resp1.content, 'html.parser')\n",
    "                            \n",
    "                            # Extract salary\n",
    "                            salary = np.nan\n",
    "                            a_tag_salary = new_soup.find(\"a\", title=lambda x: x and \"Salaries\" in x)\n",
    "                            if a_tag_salary:\n",
    "                                salary_elem = a_tag_salary.find(\"div\", class_=\"text-primary-text font-pn-600 text-xs\")\n",
    "                                if salary_elem:\n",
    "                                    salary = salary_elem.text.strip()\n",
    "                            dictionary[\"salary\"].append(salary)\n",
    "                            \n",
    "                            # Extract job count\n",
    "                            job = np.nan\n",
    "                            a_tag_job = new_soup.find(\"a\", title=lambda x: x and \"Jobs\" in x)\n",
    "                            if a_tag_job:\n",
    "                                job_elem = a_tag_job.find(\"div\", class_=\"text-primary-text font-pn-600 text-xs\")\n",
    "                                if job_elem:\n",
    "                                    job = job_elem.text.strip()\n",
    "                            dictionary[\"job\"].append(job)\n",
    "                            \n",
    "                            # Extract founded year\n",
    "                            founded_year = np.nan\n",
    "                            year_elements = new_soup.find_all(\"div\", \n",
    "                                class_=\"inline whitespace-pre-wrap break-words text-primary-text text-sm font-pn-600 flex-[6] md:flex-[auto]\")\n",
    "                            if year_elements and len(year_elements) > 0:\n",
    "                                founded_year = year_elements[0].text.strip()\n",
    "                            dictionary[\"founded_in\"].append(founded_year)\n",
    "                            \n",
    "                        else:\n",
    "                            dictionary[\"salary\"].append(np.nan)\n",
    "                            dictionary[\"job\"].append(np.nan)\n",
    "                            dictionary[\"founded_in\"].append(np.nan)\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error scraping {company_name}: {e}\")\n",
    "                        dictionary[\"salary\"].append(np.nan)\n",
    "                        dictionary[\"job\"].append(np.nan)\n",
    "                        dictionary[\"founded_in\"].append(np.nan)\n",
    "                else:\n",
    "                    dictionary[\"salary\"].append(np.nan)\n",
    "                    dictionary[\"job\"].append(np.nan)\n",
    "                    dictionary[\"founded_in\"].append(np.nan)\n",
    "            \n",
    "            time.sleep(2)\n",
    "        \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error accessing page {page} for {city}: {e}\")\n",
    "            break\n",
    "    \n",
    "    return pd.DataFrame(dictionary)\n",
    "\n",
    "\n",
    "def create_excel_report(city_dfs, output_dir):\n",
    "    \"\"\"Create comprehensive Excel report with multiple sheets\"\"\"\n",
    "    import datetime\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    excel_file = os.path.join(output_dir, f\"Company_Data_Report_{timestamp}.xlsx\")\n",
    "    \n",
    "    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "        # Create summary sheet\n",
    "        summary_data = []\n",
    "        \n",
    "        for city_name, df in city_dfs.items():\n",
    "            if not df.empty:\n",
    "                summary_data.append({\n",
    "                    'City': city_name.replace('-', ' ').title(),\n",
    "                    'Total Companies': len(df),\n",
    "                    'Average Rating': round(df['rating'].mean(), 2),\n",
    "                    'Average Salary (‚Çπ)': round(df['salary'].mean(), 2),\n",
    "                    'Total Jobs': int(df['job'].sum()),\n",
    "                    'Top Rated Company': df.loc[df['rating'].idxmax(), 'company_name'][:30] if 'company_name' in df.columns else 'N/A',\n",
    "                    'Highest Salary (‚Çπ)': round(df['salary'].max(), 2)\n",
    "                })\n",
    "                \n",
    "                # Save individual city sheet\n",
    "                sheet_name = city_name[:31]  # Excel sheet name max 31 chars\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "        # Create summary DataFrame\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Create analysis sheet with top companies\n",
    "        all_data = pd.concat(city_dfs.values(), ignore_index=True)\n",
    "        all_data['city'] = [city for city, df in city_dfs.items() for _ in range(len(df))]\n",
    "        \n",
    "        # Top 20 companies by rating\n",
    "        top_20_by_rating = all_data.nlargest(20, 'rating')[['company_name', 'city', 'rating', 'salary', 'job', 'founded_in']]\n",
    "        top_20_by_rating.to_excel(writer, sheet_name='Top 20 Companies', index=False)\n",
    "        \n",
    "        # Top 20 companies by salary\n",
    "        top_20_by_salary = all_data.nlargest(20, 'salary')[['company_name', 'city', 'rating', 'salary', 'job', 'founded_in']]\n",
    "        top_20_by_salary.to_excel(writer, sheet_name='Top Salaries', index=False)\n",
    "    \n",
    "    # Format the Excel file\n",
    "    format_excel_file(excel_file, 'Summary')\n",
    "    \n",
    "    print(f\"\\n‚úÖ Excel report created: {excel_file}\")\n",
    "    return excel_file\n",
    "\n",
    "\n",
    "def main():\n",
    "    cities = [\"new-delhi\", \"mumbai\", \"pune\", \"bangalore\", \"chennai\", \"noida\", \"jaipur\", \"surat\"]\n",
    "    \n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = \"excel_reports\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    city_dataframes = {}\n",
    "    \n",
    "    # Step 1: Scrape data for each city\n",
    "    for city in cities:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Scraping data for {city}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            df = scrape_city_data(city, headers, max_pages=11)\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Clean and process data\n",
    "                df[\"job\"] = df[\"job\"].apply(clean_salary_job)\n",
    "                df[\"salary\"] = df[\"salary\"].apply(clean_salary_job)\n",
    "                \n",
    "                # Fill missing values with median\n",
    "                if df[\"job\"].notna().any():\n",
    "                    median_job = df[\"job\"].median()\n",
    "                    df[\"job\"].fillna(median_job, inplace=True)\n",
    "                \n",
    "                if df[\"salary\"].notna().any():\n",
    "                    median_salary = df[\"salary\"].median()\n",
    "                    df[\"salary\"].fillna(median_salary, inplace=True)\n",
    "                \n",
    "                # Split bio column\n",
    "                df[[\"field\", \"other\"]] = df[\"bio\"].str.split(\"|\", n=1, expand=True)\n",
    "                df[\"field\"] = df[\"field\"].str.strip() if df[\"field\"].notna().any() else df[\"field\"]\n",
    "                df[\"other\"] = df[\"other\"].str.strip() if df[\"other\"].notna().any() else df[\"other\"]\n",
    "                \n",
    "                # Save individual Excel file for this city\n",
    "                city_excel = os.path.join(output_dir, f\"{city}_companies.xlsx\")\n",
    "                df.to_excel(city_excel, index=False)\n",
    "                format_excel_file(city_excel)\n",
    "                print(f\"‚úÖ Individual Excel file saved: {city_excel}\")\n",
    "                \n",
    "                # Store for combined report\n",
    "                city_dataframes[city] = df\n",
    "                \n",
    "            else:\n",
    "                print(f\"No data scraped for {city}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {city}: {e}\")\n",
    "    \n",
    "    # Step 2: Create comprehensive Excel report\n",
    "    if city_dataframes:\n",
    "        excel_report = create_excel_report(city_dataframes, output_dir)\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"üìä EXCEL REPORTS CREATED SUCCESSFULLY!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for city, df in city_dataframes.items():\n",
    "            print(f\"\\n{city.replace('-', ' ').title():15} | Companies: {len(df):4} | Avg Rating: {df['rating'].mean():.2f} | Avg Salary: ‚Çπ{df['salary'].mean():,.0f}\")\n",
    "        \n",
    "        print(f\"\\nüìÅ All Excel files are saved in: {os.path.abspath(output_dir)}\")\n",
    "        print(f\"üìä Main report file: {excel_report}\")\n",
    "        \n",
    "        # Open Excel file automatically (Windows only)\n",
    "        try:\n",
    "            os.startfile(excel_report)\n",
    "            print(\"üìÇ Excel file opened automatically!\")\n",
    "        except:\n",
    "            print(\"\\nüìÇ To open the Excel file, navigate to:\")\n",
    "            print(f\"   {os.path.abspath(excel_report)}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No data was scraped. Please check your internet connection or the website.\")\n",
    "\n",
    "\n",
    "def view_excel_data():\n",
    "    \"\"\"View the created Excel files\"\"\"\n",
    "    import subprocess\n",
    "    import glob\n",
    "    \n",
    "    output_dir = \"excel_reports\"\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"‚ùå No Excel reports found. Please run the scraper first.\")\n",
    "        return\n",
    "    \n",
    "    excel_files = glob.glob(os.path.join(output_dir, \"*.xlsx\"))\n",
    "    \n",
    "    if not excel_files:\n",
    "        print(\"‚ùå No Excel files found in the reports directory.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"üìÅ AVAILABLE EXCEL FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i, file in enumerate(sorted(excel_files), 1):\n",
    "        file_size = os.path.getsize(file) / 1024  # Size in KB\n",
    "        print(f\"{i:2}. {os.path.basename(file):40} ({file_size:.1f} KB)\")\n",
    "    \n",
    "    print(f\"\\nTotal Excel files: {len(excel_files)}\")\n",
    "    \n",
    "    while True:\n",
    "        choice = input(\"\\nEnter file number to open, 'a' to open all, or 'q' to quit: \").lower()\n",
    "        \n",
    "        if choice == 'q':\n",
    "            break\n",
    "        elif choice == 'a':\n",
    "            for file in excel_files:\n",
    "                try:\n",
    "                    os.startfile(file)\n",
    "                    print(f\"Opened: {os.path.basename(file)}\")\n",
    "                except:\n",
    "                    print(f\"Could not open: {os.path.basename(file)}\")\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                idx = int(choice) - 1\n",
    "                if 0 <= idx < len(excel_files):\n",
    "                    file_to_open = excel_files[idx]\n",
    "                    try:\n",
    "                        os.startfile(file_to_open)\n",
    "                        print(f\"‚úÖ Opened: {os.path.basename(file_to_open)}\")\n",
    "                        \n",
    "                        # Preview file contents\n",
    "                        df = pd.read_excel(file_to_open, sheet_name=None)\n",
    "                        print(f\"\\nüìã File contains {len(df)} sheet(s):\")\n",
    "                        for sheet_name, sheet_data in df.items():\n",
    "                            print(f\"   - {sheet_name}: {len(sheet_data)} rows √ó {len(sheet_data.columns)} columns\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Could not open file: {e}\")\n",
    "                else:\n",
    "                    print(\"‚ùå Invalid number. Please try again.\")\n",
    "            except ValueError:\n",
    "                print(\"‚ùå Invalid input. Please enter a number, 'a', or 'q'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üè¢ AMBITIONBOX COMPANY DATA SCRAPER\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nOptions:\")\n",
    "    print(\"1. Scrape data and create Excel reports\")\n",
    "    print(\"2. View existing Excel reports\")\n",
    "    print(\"3. Both (scrape first, then view)\")\n",
    "    \n",
    "    choice = input(\"\\nSelect option (1, 2, or 3): \").strip()\n",
    "    \n",
    "    if choice in [\"1\", \"3\"]:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üöÄ STARTING DATA SCRAPING\")\n",
    "        print(\"=\"*60)\n",
    "        main()\n",
    "    \n",
    "    if choice in [\"2\", \"3\"]:\n",
    "        view_excel_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
